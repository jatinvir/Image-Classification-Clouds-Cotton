{"cells":[{"attachments":{},"cell_type":"markdown","id":"c0451b95-11b7-422d-9a6f-3af262aeac03","metadata":{},"source":["# Image Classification: Cotton Candy vs Clouds"]},{"cell_type":"markdown","id":"41832db2-0d13-4f48-a618-3a08679147e9","metadata":{"tags":[]},"source":["# Package Imports"]},{"cell_type":"code","execution_count":null,"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","metadata":{"executionCancelledAt":null,"executionTime":53,"id":"bA5ajAmk7XH6","lastExecutedAt":1694359871853,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Package imports go here\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport time\nimport os\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nfrom torchvision import datasets, models, transforms"},"outputs":[],"source":["# Package imports go here\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import time\n","import os\n","import copy\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","from torchvision import datasets, models, transforms"]},{"attachments":{},"cell_type":"markdown","id":"e0f0b603-0218-419d-be23-211feca08b0f","metadata":{},"source":["## Initalizations\n","For faster runtime"]},{"cell_type":"code","execution_count":null,"id":"aa3acbb8-eb1d-4a97-9d6b-fe4e491ba21c","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1694359871901,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Enable cudnn benchmark\ncudnn.benchmark = True"},"outputs":[],"source":["# Enable cudnn benchmark\n","cudnn.benchmark = True"]},{"cell_type":"markdown","id":"f8d6f06d-dcf8-40b4-9398-98ece40a3f8c","metadata":{},"source":["# Reading and transforming the data"]},{"attachments":{},"cell_type":"markdown","id":"1ed5ce76-2cad-428e-b1d9-466935b9745b","metadata":{},"source":["Lading and transforming our data by defining the specific transforms we'd like to use from `torchvision`.\n"]},{"cell_type":"code","execution_count":null,"id":"b8e4238c-f4e8-4a95-8a55-8d630e73306f","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1694359873441,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create data transforms\n# images could be in any size, shape or form\n# want to standardize so that theyre all consistent\n# transform into way to help model. seperate training and val\ndata_transforms = {\n    \"train\" : transforms.Compose(\n        [\n            # resize into 224 pixels  square and randomized\n            transforms.RandomResizedCrop(224),\n            # flip them horizontally sometime, so that its random and good for new images\n            transforms.RandomHorizontalFlip(),\n            # images as np.arrays want in tensor as torch uses that\n            transforms.ToTensor(),\n            # Standardize variabiulity, sub mean / std, use these values cause resnet model trained on these means and std\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n    ),\n    \"val\" : transforms.Compose(\n        [\n            # only shape\n            transforms.Resize(256),\n            #only see center of image\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n}"},"outputs":[],"source":["# Create data transforms\n","# images could be in any size, shape or form\n","# want to standardize so that theyre all consistent\n","# transform into way to help model. seperate training and val\n","data_transforms = {\n","    \"train\" : transforms.Compose(\n","        [\n","            # resize into 224 pixels  square and randomized\n","            transforms.RandomResizedCrop(224),\n","            # flip them horizontally sometime, so that its random and good for new images\n","            transforms.RandomHorizontalFlip(),\n","            # images as np.arrays want in tensor as torch uses that\n","            transforms.ToTensor(),\n","            # Standardize variabiulity, sub mean / std, use these values cause resnet model trained on these means and std\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]\n","    ),\n","    \"val\" : transforms.Compose(\n","        [\n","            # only shape\n","            transforms.Resize(256),\n","            #only see center of image\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","}"]},{"attachments":{},"cell_type":"markdown","id":"db9704a8-c661-4ba8-b4fc-ddbe6472ffa5","metadata":{},"source":["Set the Data for training and testing"]},{"cell_type":"code","execution_count":null,"id":"35154ab3-df9d-472e-8e13-e0079b6ce87b","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1694359873489,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Provide data directory\ndata_dir = \"data/sloths_versus_pain_au_chocolat\"\n\n# Create image folders for our training and validation data \nimage_datasets = {\n    x: datasets.ImageFolder(os.path.join(data_dir, x),\n                           data_transforms[x]\n                           )\n    for x in [\"train\", \"val\"]\n}\n\n# Obtain dataset sizes from image_datasets\ndataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]}\n\n# Obtain class_names from image_datasets\nclass_names = image_datasets[\"train\"].classes\n# print(class_names)\n\n# Use image_datasets to sample from the dataset\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n                                             batch_size = 4,\n                                             shuffle=True)\n               for x in [\"train\", \"val\"]\n                                             }\n\n","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"outputs":[],"source":["# Provide data directory\n","data_dir = \"data/cotton_versus_clouds\"\n","\n","# Create image folders for our training and validation data \n","image_datasets = {\n","    x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                           data_transforms[x]\n","                           )\n","    for x in [\"train\", \"val\"]\n","}\n","\n","# Obtain dataset sizes from image_datasets\n","dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]}\n","\n","# Obtain class_names from image_datasets\n","class_names = image_datasets[\"train\"].classes\n","# print(class_names)\n","\n","# Use image_datasets to sample from the dataset\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n","                                             batch_size = 4,\n","                                             shuffle=True)\n","               for x in [\"train\", \"val\"]\n","                                             }\n","\n"]},{"cell_type":"code","execution_count":null,"id":"738a0adc-c398-4036-acd7-27a5fe4c6099","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1694359873537,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Change selected device to CUDA, a parallel processing platform, if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"},"outputs":[],"source":["# Change selected device to CUDA, a parallel processing platform, if available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"attachments":{},"cell_type":"markdown","id":"1dab08c1-ad7e-4e78-ac96-053d1b631b8b","metadata":{},"source":["# Visualizing clouds and candy with a custom function"]},{"cell_type":"markdown","id":"0f6edaec-2e78-46e3-8e5b-46fc8029ff52","metadata":{},"source":["## Defining the function"]},{"cell_type":"code","execution_count":null,"id":"3b626152-0d16-4925-b336-76d916629559","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1694359873585,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def imshow(inp, title=None):\n    \"\"\"\n    This function will make use of Matplotlib.pyplot's imshow() function for tensors. \n    It will show the same number of images as the batch we defined.\n    \"\"\"\n    # A transpose is required to get the images into the correct shape\n    inp = inp.numpy().transpose((1, 2, 0))\n\n    # Using default values for mean and std but can customize\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    \n    \n    # To visualize the correct colors, reverses normalized\n    inp = std * inp + mean\n    \n    # To view a clipped version of an image  \n    inp = np.clip(inp, 0, 1)\n    \n    # Visualize inp\n    plt.imshow(inp)\n    \n    if title is not None: # Plot title goes here\n        plt.title(title)\n       \n # Enables the function to pause while the plots are updated\nplt.pause(0.001)\n"},"outputs":[],"source":["def imshow(inp, title=None):\n","    \"\"\"\n","    This function will make use of Matplotlib.pyplot's imshow() function for tensors. \n","    It will show the same number of images as the batch we defined.\n","    \"\"\"\n","    # A transpose is required to get the images into the correct shape\n","    inp = inp.numpy().transpose((1, 2, 0))\n","\n","    # Using default values for mean and std but can customize\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    \n","    \n","    # To visualize the correct colors, reverses normalized\n","    inp = std * inp + mean\n","    \n","    # To view a clipped version of an image  \n","    inp = np.clip(inp, 0, 1)\n","    \n","    # Visualize inp\n","    plt.imshow(inp)\n","    \n","    if title is not None: # Plot title goes here\n","        plt.title(title)\n","       \n"," # Enables the function to pause while the plots are updated\n","plt.pause(0.001)\n"]},{"attachments":{},"cell_type":"markdown","id":"764994aa-14fe-4db7-9cc0-8cb22e2609ad","metadata":{},"source":["## Calling `imshow()` function"]},{"cell_type":"code","execution_count":null,"id":"93a38d3b-7dea-4d93-9bbc-018bb0f85adb","metadata":{"executionCancelledAt":null,"executionTime":447,"lastExecutedAt":1694359874033,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get a batch of training data\ninputs, classes = next(iter(dataloaders[\"train\"]))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\n# Plot the grid with a title that concatenates all the class labels\nimshow(out, title = [class_names[x] for x in classes])"},"outputs":[],"source":["# Get a batch of training data\n","inputs, classes = next(iter(dataloaders[\"train\"]))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","\n","# Plot the grid with a title that concatenates all the class labels\n","imshow(out, title = [class_names[x] for x in classes])"]},{"cell_type":"code","execution_count":null,"id":"e98ae050-2a98-4838-8161-a358cd3d99d7","metadata":{"executionCancelledAt":null,"executionTime":264,"lastExecutedAt":1694359874297,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get a batch of valudation data\ninputs, classes = next(iter(dataloaders[\"val\"]))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\n# Plot the grid with a title that concatenates all the class labels\nimshow(out, title = [class_names[x] for x in classes])"},"outputs":[],"source":["# Get a batch of validation data\n","inputs, classes = next(iter(dataloaders[\"val\"]))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","\n","# Plot the grid with a title that concatenates all the class labels\n","imshow(out, title = [class_names[x] for x in classes])"]},{"cell_type":"markdown","id":"62f4c8ea-1863-47ba-8930-254f3de8a7bc","metadata":{},"source":["# Running the model with 25 epochs"]},{"attachments":{},"cell_type":"markdown","id":"8c08be90-ff2e-49a6-97a9-231b4c31e0e3","metadata":{},"source":["## Functions for training the model and visualizing model results"]},{"cell_type":"code","execution_count":null,"id":"924b7a31-b960-4b46-a004-cc66ecb18007","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1694360059629,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    '''\n    Function that will train model based on data provided.\n    '''\n    \n    since = time.time()\n\n    # Make a deep copy of the model provided     \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data using the dataloader we defined\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # Zero the parameter gradients\n                optimizer.zero_grad()\n\n                # Forward pass, tracking history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # Backward pass and optimization only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # Computing loss statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # Create a deep copy of the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print() # Print an empty line for nice formatting\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # Load the best model weights\n    model.load_state_dict(best_model_wts)\n    return model"},"outputs":[],"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    '''\n","    Function that will train model based on data provided.\n","    '''\n","    \n","    since = time.time()\n","\n","    # Make a deep copy of the model provided     \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data using the dataloader we defined\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # Forward pass, tracking history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # Backward pass and optimization only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # Computing loss statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n","\n","            # Create a deep copy of the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print() # Print an empty line for nice formatting\n","\n","    time_elapsed = time.time() - since\n","    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n","    print(f'Best val Acc: {best_acc:4f}')\n","\n","    # Load the best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"ae373682-e4d2-4966-beea-cd47cc45c91d","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1694359874401,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def visualize_model(model, num_images=6):\n    '''\n    Function that will visualize results of the model\n    '''\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)"},"outputs":[],"source":["def visualize_model(model, num_images=6):\n","    '''\n","    Function that will visualize results of the model\n","    '''\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    fig = plt.figure()\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(dataloaders['val']):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                ax = plt.subplot(num_images//2, 2, images_so_far)\n","                ax.axis('off')\n","                ax.set_title(f'predicted: {class_names[preds[j]]}')\n","                imshow(inputs.cpu().data[j])\n","\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)"]},{"cell_type":"markdown","id":"10b25c95-1a92-47c7-8a25-70bc6bd97b75","metadata":{},"source":["## Loading a pre-trained model"]},{"attachments":{},"cell_type":"markdown","id":"ba95cc32-5e33-491f-ac93-6c3468dbb4c0","metadata":{},"source":["Use Resnet18"]},{"cell_type":"code","execution_count":null,"id":"6dba17ff-5277-49bc-820d-5f16ada11b32","metadata":{"executionCancelledAt":null,"executionTime":172,"lastExecutedAt":1694360074073,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load the resnet model\nmodel_ft = models.resnet18(pretrained=True)\n\n# Obtaining the number of input features for our final layer\n# finding the size of the input features\nnum_ftrs = model_ft.fc.in_features\n\n# Since this is a binary classification task, we'll set the size of each output sample to 2. For multi-class classification, this can be generalized to nn.Linear(num_ftrs, len(class_names)).\n# replaced with linear layer that we want, 2 is since binary classification\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\n\n# Move the model to the device\nmodel_ft = model_ft.to(device)\n\n# We'll use CrossEntropyLoss(), which is a common loss function for classification problems\n# see how well model is doing\ncreiterion = nn.CrossEntropyLoss()\n\n# In this step, we'll optimize all parameters of the model\n# use to train model\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# We'll decay learning rate (lr) by a factor of 0.1 every 7 epochs\n# fast/slow model is going\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 7, gamma = 0.1)","outputsMetadata":{"0":{"height":75,"type":"stream"}}},"outputs":[],"source":["# Load the resnet model\n","model_ft = models.resnet18(pretrained=True)\n","\n","# Obtaining the number of input features for our final layer\n","# finding the size of the input features\n","num_ftrs = model_ft.fc.in_features\n","\n","# Since this is a binary classification task, we'll set the size of each output sample to 2. For multi-class classification, this can be generalized to nn.Linear(num_ftrs, len(class_names)).\n","# replaced with linear layer that we want, 2 is since binary classification\n","model_ft.fc = nn.Linear(num_ftrs, 2)\n","\n","# Move the model to the device\n","model_ft = model_ft.to(device)\n","\n","# We'll use CrossEntropyLoss(), which is a common loss function for classification problems\n","# see how well model is doing\n","creiterion = nn.CrossEntropyLoss()\n","\n","# In this step, we'll optimize all parameters of the model\n","# use to train model\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# We'll decay learning rate (lr) by a factor of 0.1 every 7 epochs\n","# fast/slow model is going\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 7, gamma = 0.1)"]},{"cell_type":"code","execution_count":null,"id":"cd48fc8b-9583-47f8-9c1e-c571826c5ded","metadata":{"executionCancelledAt":null,"executionTime":948054,"lastExecutedAt":1694361028094,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call our train_model() function with the ResNet model, the criterion, optimizer, learning rate scheduler, and number of epochs that we have defined.\nmodel_ft = train_model(model_ft, creiterion, optimizer_ft, exp_lr_scheduler, num_epochs = 25)","outputsMetadata":{"0":{"height":599,"type":"stream"}}},"outputs":[],"source":["# Call our train_model() function with the ResNet model, the criterion, optimizer, learning rate scheduler, and number of epochs that we have defined.\n","model_ft = train_model(model_ft, creiterion, optimizer_ft, exp_lr_scheduler, num_epochs = 25)"]},{"cell_type":"code","execution_count":null,"id":"5c56c677-7e3f-4f97-a9a3-5df7bd2fe088","metadata":{"executionCancelledAt":1694359910129,"executionTime":669,"lastExecutedAt":1685461418575,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"visualize_model(model_ft)"},"outputs":[],"source":["visualize_model(model_ft)"]},{"cell_type":"code","execution_count":null,"id":"57a6cbda-17e8-4f40-bd5f-d6ed5587dfce","metadata":{"executionCancelledAt":null,"executionTime":167,"lastExecutedAt":1694362793546,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# to fine tune it\n# Disable gradients for model_conv.parameters()\nmodel_conv = torchvision.models.resnet18(pretrained=True)\nfor param in model_conv.parameters():\n    param.requires_grid = False\n\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\n\n# Move the model to the device\nmodel_conv = model_conv.to(device)\n\n\n# Set criterion again\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only parameters of final layer are being optimized as opposed to before\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_schedulr = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n"},"outputs":[],"source":["# to fine tune it for niche images\n","# Disable gradients for model_conv.parameters()\n","model_conv = torchvision.models.resnet18(pretrained=True)\n","for param in model_conv.parameters():\n","    param.requires_grid = False\n","\n","\n","# Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model_conv.fc.in_features\n","model_conv.fc = nn.Linear(num_ftrs, 2)\n","\n","\n","# Move the model to the device\n","model_conv = model_conv.to(device)\n","\n","\n","# Set criterion again\n","criterion = nn.CrossEntropyLoss()\n","\n","# Observe that only parameters of final layer are being optimized as opposed to before\n","optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n","\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_schedulr = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n"]},{"cell_type":"code","execution_count":null,"id":"6718ecd2-2476-43e9-a515-d713413ddcd3","metadata":{"executionCancelledAt":null,"executionTime":10132,"lastExecutedAt":1694362821651,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Train model_conv\nmodel_conv = train_model(model_conv, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=25)","outputsMetadata":{"0":{"height":56,"type":"stream"}}},"outputs":[],"source":["# Train model_conv\n","model_conv = train_model(model_conv, criterion, optimizer_conv,\n","                         exp_lr_scheduler, num_epochs=25)"]},{"cell_type":"code","execution_count":null,"id":"e7148824-5265-4518-95de-b64bd16a8911","metadata":{"executionCancelledAt":1694359910131},"outputs":[],"source":["# Visualize model\n","visualize_model(model_conv)\n","plt.show()"]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1 (main, Jan  9 2024, 09:39:56) [Clang 15.0.0 (clang-1500.1.0.2.5)]"},"vscode":{"interpreter":{"hash":"16b394dcf9a236256d110ccc1d794784b36abac37d8d39025d5b0c3559718f75"}}},"nbformat":4,"nbformat_minor":5}
